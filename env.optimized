# ===========================
# OPTIMIZED DOCHARVESTER CONFIG
# ===========================

# === DATABASE CONFIGURATION ===
DATABASE_URL=postgresql+asyncpg://postgres:postgres@postgres:5432/docharvester
REDIS_URL=redis://redis:6379/0
CELERY_BROKER_URL=redis://redis:6379/1
CELERY_RESULT_BACKEND=redis://redis:6379/2

# === LLM CONFIGURATION (FIXED) ===
# Primary LLM provider - set to LOCAL for reliable operation
CURRENT_LLM_PROVIDER=LOCAL
USE_LOCAL_LLM=true
LOCAL_LLM_MODEL=gemma:2b

# Ollama Configuration
OLLAMA_BASE_URL=http://ollama:11434

# OpenAI Configuration (optional - for switching if needed)
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_ORGANIZATION_ID=your-organization-id-here

# === OPTIMIZED PERFORMANCE SETTINGS ===

# LLM Model Settings
LLM_MODEL=gemma:2b
LLM_MAX_TOKENS=2000
LLM_TEMPERATURE=0.7

# Document Processing Optimization
CHUNK_SIZE=1500                    # Optimized chunk size for gemma:2b
CHUNK_OVERLAP=200                  # Balanced overlap
MAX_FILE_SIZE_MB=25               # Reduced max file size to prevent timeouts

# Entity Extraction Optimization  
ENTITY_EXTRACTION_ENABLED=true
ENTITY_EXTRACTION_TIMEOUT=90      # Reduced from 120 to 90 seconds
ENTITY_EXTRACTION_BATCH_SIZE=5    # Process entities in smaller batches

# Wiki Generation Optimization
WIKI_GENERATION_ENABLED=true
WIKI_GENERATION_TIMEOUT=120       # 2 minute timeout per page
WIKI_MAX_CHUNKS_PER_PAGE=15       # Limit chunks to prevent context overflow
WIKI_CACHE_ENABLED=true           # Enable response caching

# Worker Configuration
WORKER_BATCH_SIZE=10              # Reduced batch size
WORKER_TIMEOUT_SECONDS=1800       # 30 minutes max
MAX_CONCURRENT_TASKS=2            # Reduced concurrency for stability

# === KNOWLEDGE GRAPH CONFIGURATION ===
ENABLE_KNOWLEDGE_GRAPH=true
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your-secure-neo4j-password-here
GRAPHITI_URL=http://graphiti:8000

# === PERFORMANCE TUNING ===

# HTTP Client Optimization
HTTP_TIMEOUT_CONNECT=10
HTTP_TIMEOUT_READ=120
HTTP_TIMEOUT_WRITE=30
HTTP_MAX_CONNECTIONS=20
HTTP_MAX_KEEPALIVE=10

# Memory Management
CACHE_MAX_SIZE=100
CACHE_DEFAULT_TTL=600             # 10 minute cache TTL
CACHE_ENTITY_TTL=1800            # 30 minute entity cache

# Context Window Management
LOCAL_LLM_CONTEXT_WINDOW=4096
LOCAL_LLM_MAX_INPUT_TOKENS=3000
LOCAL_LLM_RESERVED_OUTPUT_TOKENS=1000

# === APPLICATION SETTINGS ===
DEBUG=false
LOG_LEVEL=INFO
SECRET_KEY=your-secure-secret-key-change-in-production
ACCESS_TOKEN_EXPIRE_MINUTES=30

# === DOCKER OPTIMIZATION ===
# These help with container resource management
COMPOSE_PARALLEL_LIMIT=3
COMPOSE_HTTP_TIMEOUT=60

# === MONITORING & HEALTH CHECKS ===
HEALTH_CHECK_INTERVAL=30
HEALTH_CHECK_TIMEOUT=10
HEALTH_CHECK_RETRIES=3

# === DEVELOPMENT OVERRIDES ===
# Uncomment for development with faster models
# LOCAL_LLM_MODEL=gemma:2b
# WIKI_GENERATION_TIMEOUT=60
# ENTITY_EXTRACTION_TIMEOUT=45

# === PRODUCTION OVERRIDES ===
# Uncomment for production with OpenAI
# CURRENT_LLM_PROVIDER=OPENAI
# USE_LOCAL_LLM=false
# LLM_MODEL=gpt-4o-mini
# LLM_MAX_TOKENS=4000
# CHUNK_SIZE=3000 