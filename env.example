# DocHarvester Environment Configuration
# Copy this file to .env and update with your values

# === CORE APPLICATION ===
APP_NAME=DocHarvester
APP_VERSION=1.0.0
DEBUG=false

# API Configuration
API_PREFIX=/api/v1
CORS_ORIGINS=["http://localhost:3000", "http://localhost:8000"]

# === DATABASE CONFIGURATION ===
# PostgreSQL with pgvector for embeddings
DATABASE_URL=postgresql+asyncpg://postgres:postgres@postgres:5432/docharvester

# Redis for caching and task queues
REDIS_URL=redis://redis:6379/0
CELERY_BROKER_URL=redis://redis:6379/1
CELERY_RESULT_BACKEND=redis://redis:6379/2

# === AI/LLM CONFIGURATION ===

# Dynamic LLM Provider Selection
# Can be switched via admin UI or set here
CURRENT_LLM_PROVIDER=LOCAL  # LOCAL or OPENAI

# Local LLM Configuration (Recommended for development)
# Set to true to use local Ollama models (cost-free, private)
USE_LOCAL_LLM=true
LOCAL_LLM_MODEL=gemma:2b
OLLAMA_BASE_URL=http://ollama:11434

# OpenAI Configuration (Recommended for production with large data)
# REQUIRED: Set your OpenAI API key to use cloud models
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_ORGANIZATION_ID=your-organization-id-here  # Optional: Your organization ID

# Large Context Model Selection (when using OPENAI)
# Available models with context windows:
# - gpt-4o (128k context) - Latest, most capable
# - gpt-4o-mini (128k context) - Cost-effective, fast
# - gpt-4-turbo (128k context) - High performance
# - gpt-4-turbo-preview (128k context) - Preview version
# - gpt-4 (8k context) - Standard GPT-4
# - gpt-3.5-turbo (16k context) - Cost-effective
LLM_MODEL=gpt-4o

# Azure OpenAI Configuration (Alternative to OpenAI)
# LLM_PROVIDER=AZURE_OPENAI
# AZURE_OPENAI_API_KEY=your-azure-key-here
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_DEPLOYMENT=your-deployment-name

# === KNOWLEDGE GRAPH CONFIGURATION ===
# Enable entity extraction and knowledge graph features
ENABLE_KNOWLEDGE_GRAPH=true

# Neo4j Configuration
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your-secure-neo4j-password-here

# === DOCUMENT PROCESSING ===
# Text chunking configuration - optimized for large context models
CHUNK_SIZE=2000    # Increased for better context with large models
CHUNK_OVERLAP=400  # Higher overlap for better coherence

# File upload limits
MAX_FILE_SIZE_MB=100  # Increased for large document processing
ALLOWED_EXTENSIONS=[".txt", ".md", ".pdf", ".docx", ".html", ".json", ".yml", ".yaml", ".py", ".js", ".ts"]

# === PERFORMANCE & SCALING ===
# Worker configuration
WORKER_BATCH_SIZE=10        # Reduced for better memory management with large models
WORKER_TIMEOUT_SECONDS=600  # Increased for large document processing
MAX_CONCURRENT_TASKS=2      # Reduced for better resource management

# LLM Settings (for both local and cloud)
LLM_TEMPERATURE=0.3         # Lower for more consistent results
LLM_MAX_TOKENS=4000         # Increased for large context models

# Embedding Configuration
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSION=1536

# === SECURITY ===
# Change this to a secure random key in production!
SECRET_KEY=your-secure-secret-key-change-in-production

# Token expiration (in minutes)
ACCESS_TOKEN_EXPIRE_MINUTES=1440

# === FEATURES ===
# Coverage analysis configuration
COVERAGE_CONFIG_PATH=coverage.yml

# Entity extraction configuration
ENTITY_EXTRACTION_ENABLED=true
ENTITY_EXTRACTION_TIMEOUT=120  # Increased for large documents

# Wiki generation
WIKI_GENERATION_ENABLED=true

# === DOCKER CONFIGURATION ===
# These settings are used when running in Docker
# For local development, change URLs to localhost

# Docker database URL
# DATABASE_URL=postgresql+asyncpg://postgres:postgres@postgres:5432/docharvester

# Docker Redis URLs  
# REDIS_URL=redis://redis:6379/0
# CELERY_BROKER_URL=redis://redis:6379/1
# CELERY_RESULT_BACKEND=redis://redis:6379/2

# === DEVELOPMENT OVERRIDES ===
# For local development outside Docker, uncomment these:
# DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/docharvester
# REDIS_URL=redis://localhost:6379/0
# CELERY_BROKER_URL=redis://localhost:6379/1
# CELERY_RESULT_BACKEND=redis://localhost:6379/2
# OLLAMA_BASE_URL=http://localhost:11434
# NEO4J_URI=bolt://localhost:7687

# === RECOMMENDED CONFIGURATIONS ===

# For Development (Local LLM):
# CURRENT_LLM_PROVIDER=LOCAL
# USE_LOCAL_LLM=true
# LOCAL_LLM_MODEL=gemma:2b
# ENABLE_KNOWLEDGE_GRAPH=true

# For Production with Large Documents (OpenAI):
# CURRENT_LLM_PROVIDER=OPENAI
# OPENAI_API_KEY=your-production-key
# LLM_MODEL=gpt-4o
# LLM_MAX_TOKENS=4000
# CHUNK_SIZE=2000
# ENABLE_KNOWLEDGE_GRAPH=true

# For Cost-Effective Large Context (OpenAI):
# CURRENT_LLM_PROVIDER=OPENAI
# OPENAI_API_KEY=your-key
# LLM_MODEL=gpt-4o-mini
# LLM_MAX_TOKENS=2000
# ENABLE_KNOWLEDGE_GRAPH=true

# For Hybrid (Local + Cloud on-demand):
# CURRENT_LLM_PROVIDER=LOCAL (switch via UI as needed)
# USE_LOCAL_LLM=true
# OPENAI_API_KEY=your-key (for manual switching)
# ENABLE_KNOWLEDGE_GRAPH=true 

# === Model Context Window Information ===
# Local Models:
# - gemma:2b (2k context) - Memory efficient, fits in 8GB+ RAM
# - gemma:7b (4k context) - Better quality, needs 16GB+ RAM
# - llama3:8b (8k context) - High quality, needs 16GB+ RAM
# - mistral:7b (8k context) - Good reasoning, needs 16GB+ RAM

# OpenAI Models (Large Context):
# - gpt-4o (128k context) - Best overall, handles massive documents
# - gpt-4o-mini (128k context) - Cost-effective large context
# - gpt-4-turbo (128k context) - High performance
# - gpt-3.5-turbo (16k context) - Budget option

# === Usage Guidelines ===
# Large Context Benefits:
# - Process entire documents at once (up to 128k tokens = ~96k words)
# - Better understanding of document structure and relationships
# - More accurate entity extraction across document sections
# - Superior wiki generation with full document context

# When to use Large Context Models:
# - Documents > 10 pages
# - Complex technical documentation
# - Legal documents requiring full context
# - Academic papers with references
# - Code repositories with multiple files 